---
title: "Results"
author: "Thomas Guillerme"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_width: 12
    fig_height: 6
---

This script is the repeatable script for measuring changes in the different diversity metrics and how they are affected by the different stressors. 
In this section we are going to analyse the results from the `optim.replicates` chain

```{r, echo = FALSE}
library(knitr)
## Loading the results
load("../Data/Processed/sim_equalizing_2d.rda")
equalizing_2d <- bkp
load("../Data/Processed/sim_facilitation_2d.rda")
facilitation_2d <- bkp
load("../Data/Processed/sim_filtering_2d.rda")
filtering_2d <- bkp
load("../Data/Processed/sim_competition_2d.rda")
competition_2d <- bkp

## Loading the results
load("../Data/Processed/sim_equalizing_3d.rda")
equalizing_3d <- bkp
load("../Data/Processed/sim_facilitation_3d.rda")
facilitation_3d <- bkp
load("../Data/Processed/sim_filtering_3d.rda")
filtering_3d <- bkp
load("../Data/Processed/sim_competition_3d.rda")
competition_3d <- bkp

## Loading empirical results
load("../Data/Processed/empirical_data-hist-present-native.rda")
empirical_hist <- bkp
load("../Data/Processed/empirical_data-prehist-present-native.rda")
empirical_prehist <- bkp

hist_table <- empirical_hist$results_table
colnames(hist_table) <- gsub("_rm1", "_rm2", colnames(hist_table))
combined_empirical <- list()
combined_empirical$results_table <- cbind(empirical_prehist$results_table, hist_table)

## Loading the functions
source("../Functions/extract.table.R")
source("../Functions/plot.metric.results.R")
source("../Functions/paired.tests.R")
source("../Functions/split.metric.R")
source("../Functions/fit.model.rm.R")
source("../Functions/fancy.table.R")
source("../Functions/fancy.plot.R")
source("../Functions/fancy.names.R")
```

# Results per metrics

Here I'm going to display the results in box plot for each metric in each broad category.
> We can probably think of a better way to display the results in the future.

For each metric displayed here:
 
 * The different colours correspond to the different stressors (equalizing, facilitation, filtering and competition)
 * The different density of each colours correspond to the different levels of each stressor (removing 20, 40, 60 or 80% of the data)
 * All the metrics values are centred on the null (random) stressor for each individual simulation (i.e. 1 dataset is simulated, X% of the data is removed, one time randomly, one time biased - following the stressor; the metric is calculated on both the random and the biased removal and the biased removal is then centred on the random one (`biased metric score - random metric score`) for each simulation).
 * All the centred metric values are then scaled to the absolute maximum change (`centred scores/max(abs(centred scores))`).

```{r, echo = FALSE}
## Creating the list of results
all_results_2d <- list(equalizing   = equalizing_2d,
                       facilitation = facilitation_2d,
                       filtering    = filtering_2d,
                       competition  = competition_2d)
all_results_3d <- list(equalizing   = equalizing_3d,
                       facilitation = facilitation_3d,
                       filtering    = filtering_3d,
                       competition  = competition_3d)
empirical_results <- list(empirical = combined_empirical)

## Sort the results per name
sort.metrics <- function(one_results, order) {

    ## Get the results table
    all_results <- colnames(one_results$results_table)

    ## Extract the metric names
    metrics <- unique(gsub("stressor_", "", gsub("random_", "", gsub("_rm.*", "", all_results))))  

    ## Reorder these names
    metrics <- metrics[order]

    ## Get the removal levels
    levels <- unique(gsub(".*_rm", "", all_results))

    ## Sort the metrics_names
    sorted_metrics <- unlist(lapply(as.list(metrics), function(x, levels) paste(x, levels, sep = "_rm"), levels = levels))
    ## Adding the random/stressor bits
    sorted_metrics <- c(apply(cbind(paste0("random_", sorted_metrics), paste0("stressor_", sorted_metrics)), 1, c))

    ## Match the new column order
    one_results$results_table <- one_results$results_table[, match(sorted_metrics, all_results)]
    
    ## Return the updated results
    return(one_results)
}

## Re-ordering results for fancyness
new_order <- c(14, 12, 3, 6,
               15, 11, 2, 8,
               7, 4, 5, 13,
               10, 9, 1)
all_results_2d <- lapply(all_results_2d, sort.metrics, order = new_order)
all_results_3d <- lapply(all_results_3d, sort.metrics, order = new_order)
empirical_results <- lapply(empirical_results, sort.metrics, order = new_order)

## The different removal level names
removals <- c("20%", "40%", "60%", "80%")
## The colours
cols <- c("blue", "orange", "red", "green")

## Fancier names
fancy_names <- c("TPD[TPD]\nregularity",
                 "hypervolume[BAT]\nregularity",
                 "dendrogram[BAT]\nregularity",
                 "dissimilarity[FD]\nregularity",
                 "TPD[TPD]\ndivergence",
                 "hypervolume[BAT]\ndivergence",
                 "dendrogram[BAT]\ndivergence",
                 "dissimilarity[melodicMPD]\ndivergence(MPD)",
                 "dissimilarity[melodicRao]\ndivergence(Rao)",
                 "dissimilarity[FD]\ndivergence(Rao)",
                 "dissimilarity[FD]\ndivergence",
                 "TPD[TPD]\nrichness",
                 "hypervolume[BAT]\nrichness",
                 "convex hull[BAT]\nrichness",
                 "dendrogram[BAT]\nrichness")
```

## Scaling and centring the results

We centre the metric based on the null results by substracting the score of the metric from the randomly reduced trait space to the stressed trait space (i.e. $score_{centred} = score_{stressed} - score_{random}$).
We then scale the centred metric by dividing it by it's maximum absolute centred value between all stressors (i.e. $score_{scaled} = score_{centred} / \text{max}(|scores_{centred}|))$).

```{r, echo = FALSE}
## Extracting all the scaled results
scaled_results_2d <- extract.table(all_results_2d, scale.method = "between")
scaled_within_2d  <- extract.table(all_results_2d, scale.method = "within")
scaled_results_3d <- extract.table(all_results_3d, scale.method = "between")
scaled_within_3d  <- extract.table(all_results_3d, scale.method = "within")
scaled_results_emp<- extract.table(empirical_results, scale.method = "between")
scaled_within_emp <- extract.table(empirical_results, scale.method = "within")
```

## Measuring the statistics for each metric

To measure the differences between the random and stressed metric scores, we ran paired t-tests between each pairs of unscaled metric score for the random reduction of the trait space and the stressed reduction of the trait space (`t.test(random_scores, stressor_scores, paired = TRUE)`).

To check the trend in the different level of data removals (20, 40, 60 and 80%) using a linear model with the $scores_{scaled}$ as a response to the different removal levels (`lm(scaled_metric ~ removal_level`).

```{r, echo = FALSE}
## Slope and R2 for each model
model_fits_2d  <- lapply(scaled_results_2d, fit.model.rm)
model_fits_3d  <- lapply(scaled_results_3d, fit.model.rm)
model_fits_emp <- lapply(scaled_results_emp, fit.model.rm)

## Differences between stressor and random
null_differences_2d  <- lapply(all_results_2d, paired.tests)
null_differences_3d  <- lapply(all_results_3d, paired.tests)
null_differences_emp <- lapply(empirical_results, paired.tests)

## Making the fancy tables
model_table_2d  <- fancy.table(model_fits_2d, columns = list(c(3,4), 5))
model_table_3d  <- fancy.table(model_fits_3d, columns = list(c(3,4), 5))
model_table_emp <- fancy.table(model_fits_emp, columns = list(c(3,4), 5))
null_table_2d   <- fancy.table(null_differences_2d, columns = list(c(1,3), c(4, 6), c(7, 9), c(10, 12)))
null_table_3d   <- fancy.table(null_differences_3d, columns = list(c(1,3), c(4, 6), c(7, 9), c(10, 12)))
null_table_emp  <- fancy.table(null_differences_emp, columns = list(c(1,3), c(4, 6)))
```

```{r, echo = FALSE}
rownames(model_table_2d) <- gsub("\\]", "_", gsub("\\[", "_", gsub("\n", "", fancy_names)))
kable(model_table_2d, caption = "Results of the model scaled metric ~ removal level per stressor (2D)", align = "c")
```
```{r, echo = FALSE}
rownames(model_table_3d) <- gsub("\\]", "_", gsub("\\[", "_", gsub("\n", "", fancy_names)))
kable(model_table_3d, caption = "Results of the model scaled metric ~ removal level per stressor (3D)", align = "c")
```
```{r, echo = FALSE}
rownames(model_table_emp) <- gsub("\\]", "_", gsub("\\[", "_", gsub("\n", "", fancy_names)))
kable(model_table_emp, caption = "Results of the model scaled metric ~ removal level for the empirical data (3D)", align = "c")
```
```{r, echo = FALSE}
rownames(null_table_2d) <- gsub("\\]", "_", gsub("\\[", "_", gsub("\n", "", fancy_names)))
## Even fancier column names
colnames(null_table_2d) <- gsub(".d_rm", ".rm", colnames(null_table_2d))
colnames(null_table_2d) <- gsub("equalizing", "equa", colnames(null_table_2d))
colnames(null_table_2d) <- gsub("facilitation", "faci", colnames(null_table_2d))
colnames(null_table_2d) <- gsub("filtering", "filt", colnames(null_table_2d))
colnames(null_table_2d) <- gsub("competition", "comp", colnames(null_table_2d))
kable(null_table_2d, caption = "Average difference between the null metric and the stressed metric (raw) for each level of removal and each stressor (2d)", align = "c")
```

```{r, echo = FALSE}
rownames(null_table_3d) <- gsub("\\]", "_", gsub("\\[", "_", gsub("\n", "", fancy_names)))
## Even fancier column names
colnames(null_table_3d) <- gsub(".d_rm", ".rm", colnames(null_table_3d))
colnames(null_table_3d) <- gsub("equalizing", "equa", colnames(null_table_3d))
colnames(null_table_3d) <- gsub("facilitation", "faci", colnames(null_table_3d))
colnames(null_table_3d) <- gsub("filtering", "filt", colnames(null_table_3d))
colnames(null_table_3d) <- gsub("competition", "comp", colnames(null_table_3d))
kable(null_table_3d, caption = "Average difference between the null metric and the stressed metric (raw) for each level of removal and each stressor (3d)", align = "c")
```

```{r, echo = FALSE}
rownames(null_table_emp) <- gsub("\\]", "_", gsub("\\[", "_", gsub("\n", "", fancy_names)))
## Even fancier column names
colnames(null_table_emp) <- gsub(".d_rm", ".rm", colnames(null_table_emp))
colnames(null_table_emp) <- gsub("equalizing", "equa", colnames(null_table_emp))
colnames(null_table_emp) <- gsub("facilitation", "faci", colnames(null_table_emp))
colnames(null_table_emp) <- gsub("filtering", "filt", colnames(null_table_emp))
colnames(null_table_emp) <- gsub("competition", "comp", colnames(null_table_emp))
kable(null_table_emp, caption = "Average difference between the null metric and the stressed metric (raw) for each level of removal for the empirical data (pre-history and history)", align = "c")
```

# Plots per stressor

```{r, fig.width = 8, fig.height = 10, echo = FALSE}
col_vector <- c(rep("blue", 4), # The richness ones
                rep("orange", 7), # The divergence ones
                rep("darkgreen", 4)) # The regularity ones
## The plot
fancy.plot(scaled_results_2d, col = col_vector, lm = model_fits_2d, null = null_differences_2d, metric.names = fancy_names)

## Pdf version
pdf(file = "../Manuscript/Figures/results_per_metric_per_stressor_2d.pdf", height = 9, width = 9)
fancy.plot(scaled_results_2d, col = col_vector, lm = model_fits_2d, null = null_differences_2d, metric.names = fancy_names)
dev.off()
```

Figure 1: Simulation results: the y axes represent the different metrics tested (sorted by categories). The different columns represent the different stressors. The x-axes represent the metric values centred on the random changes and scaled by the maximum value for each metric between the four stressors. Negative and positive values signify a decrease/increase in the metric score. The dots represent the median metric value, the full line their 50% confidence interval (CI) and the dashed line their 95% CI. The colours are just here to visually separate the metrics rows but the colour gradient within each row corresponds to a removal of respectively 80%, 60%, 40% and 20% of the data (from top to bottom). Grey lines in the background are a fitted linear model on the scaled metric score function of removal amount and the value displayed is the adjusted R^2 from each of these models. Dashed grey lines represent non-significant models (slope or/and intercept). The grey line plots represent distribution of metrics scores not clearly different from the random metric scores (paired t-test p value > 0.05).


```{r}
## The plot
fancy.plot(scaled_results_3d, col = col_vector, lm = model_fits_3d, null = null_differences_3d, metric.names = fancy_names)

## Pdf version
pdf(file = "../Manuscript/Figures/results_per_metric_per_stressor_3d.pdf", height = 9, width = 9)
fancy.plot(scaled_results_3d, col = col_vector, lm = model_fits_3d, null = null_differences_3d, metric.names = fancy_names)
dev.off()
```



```{r}
## The plot
fancy.plot(scaled_results_emp, col = col_vector, lm = model_fits_emp, null = null_differences_emp, metric.names = fancy_names)

## Pdf version
pdf(file = "../Manuscript/Figures/results_per_metric_per_stressor_emp.pdf", height = 9, width = 4)
fancy.plot(scaled_results_emp, col = col_vector, lm = model_fits_emp, null = null_differences_emp, metric.names = rev(fancy_names))
dev.off()
```





# Results interpretations in the light of our expectation table

These were our expectations:

Mechanism | Richness| Dispersion | Regularity
-------------|-------|-------|-------|
Equalizing   | Lower | Lower | Higher|
Facilitation | Higher| Higher| Higher|
Filtering (exclusion) | Higher| Higher| Higher|
Competition  | Lower | Lower |Nothing|

And these are our results per metric family: 

 * [ok] prediction is correct (lower = decrease; higher = increase)
 * [-ok] prediction is kinda correct (there is a change compared to the null but not in the right direction)
 * [NO] to a wrong prediction: no change when change is expected or change when no change is expected

> NOTE: the difference between [ok] and [-ok] is actually not really meaningful here since these graphs shows differences compared to the null results so if both the null and stressor results increase but the stressor increases slowlier than the stressor, the results in the table will decrease. Although in general, if this is the case, this is a "bad" results since ideally we would want the null to not change at all (i.e. if the metric is just picking up changes in number of elements, this is not really useful).

#### Tree (dissimilarity)

Mechanism    | Richness (alpha)| Dispersion | Regularity
-------------|------------|-------------|-------|
Equalizing   | [ok] Lower | [ok] Lower  | [-ok] Higher|
Facilitation | [NO] Higher| [-ok] Higher| [-ok] Higher|
Exclusion    | [NO] Higher| [-ok] Higher| [-ok] Higher|
Competition  | [ok] Lower | [ok] Lower  | [NO] Nothing|

#### FD

Mechanism | Richness (Rao)| Dispersion | Regularity
-------------|-------|-------|-------|
Equalizing   | [NO] Lower  | [ok] Lower | [-ok] Higher|
Facilitation | [NO] Higher | [-ok] Higher| [NO] Higher|
Exclusion    | [-ok] Higher| [-ok] Higher| [-ok] Higher|
Competition  | [ok] Lower  | [ok] Lower | [NO] Nothing|

#### hypervolume

Mechanism | Richness (Rao)| Dispersion | Regularity
-------------|-------|-------|-------|
Equalizing   | [NO] Lower  | [-ok] Lower | [-ok] Higher|
Facilitation | [-ok] Higher| [NO] Higher| [NO] Higher|
Exclusion    | [NO] Higher | [NO] Higher| [NO] Higher|
Competition  | [ok] Lower  | [NO] Lower | [ok] Nothing|


#### TPD

Mechanism | Richness (Rao)| Dispersion | Regularity
-------------|-------|-------|-------|
Equalizing   | [NO] Lower  | [ok] Lower | [-ok] Higher|
Facilitation | [NO] Higher | [-ok] Higher| [-ok] Higher|
Exclusion    | [NO] Higher | [-ok] Higher| [-ok] Higher|
Competition  | [NO] Lower  | [ok] Lower | [NO] Nothing|

#### Other

I actually don't remember of the top of my head what the convex.hull and melodic rao/mpd are representing.






# Metrics correlations

And finally below are all the correlations per stressor between each metric per removal levels (20, 40, 60, 80%).

> TODO: do the PCA plots as suggested by Carlos to see things clearlier.


```{r, eval = TRUE, echo = FALSE, message = FALSE}
require(psych)

## Split the results per stressor and per metric
## (to extract the different removal levels)
splitted_results <- lapply(scaled_results, split.metric)

## Get the right removal level
get.matrix.level <- function(results, level = 1) {
    do.call(cbind, lapply(results, function(X, level) return(X[, level, drop = FALSE]), level = level))
}
## Wrapper for pairs.panels (with fancy names)
pairs.panels.wrapper <- function(results, metric.names, ...) {
    ## Make fancy names
    if(missing(metric.names)) {
        colnames(results) <- fancy.names(colnames(results), sep = ":\n")
    } else {
        colnames(results) <- metric.names
    }
    psych::pairs.panels(results, ...)
}

```

## Equalizing

### Metrics correlation for equalizing with 20% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$equalizing, level = 1), metric.names = fancy_names)
```

### Metrics correlation for equalizing with 40% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$equalizing, level = 2), metric.names = fancy_names)
```

### Metrics correlation for equalizing with 60% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$equalizing, level = 3), metric.names = fancy_names)
```

### Metrics correlation for equalizing with 80% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$equalizing, level = 4), metric.names = fancy_names)
```

## Facilitation

### Metrics correlation for facilitation with 20% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$facilitation, level = 1), metric.names = fancy_names)
```

### Metrics correlation for facilitation with 40% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$facilitation, level = 2), metric.names = fancy_names)
```

### Metrics correlation for facilitation with 60% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$facilitation, level = 3), metric.names = fancy_names)
```

### Metrics correlation for facilitation with 80% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$facilitation, level = 4), metric.names = fancy_names)
```

## Filtering

### Metrics correlation for filtering with 20% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$filtering, level = 1), metric.names = fancy_names)
```

### Metrics correlation for filtering with 40% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$filtering, level = 2), metric.names = fancy_names)
```

### Metrics correlation for filtering with 60% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$filtering, level = 3), metric.names = fancy_names)
```

### Metrics correlation for filtering with 80% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$filtering, level = 4), metric.names = fancy_names)
```

## Competition

### Metrics correlation for competition with 20% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$competition, level = 1), metric.names = fancy_names)
```

### Metrics correlation for competition with 40% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$competition, level = 2), metric.names = fancy_names)
```

### Metrics correlation for competition with 60% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$competition, level = 3), metric.names = fancy_names)
```

### Metrics correlation for competition with 80% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = FALSE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$competition, level = 4), metric.names = fancy_names)
```













# Responses

Carlos:
-TPD richness potential error: 
  * I have double checked the scaling and everything seems OK (but I might be mistaken!) The centreing logic happens here: https://github.com/TGuillerme/eco_metrics_simulations/blob/c2a8deb9d673e0456f315d9a9f43285aafdfea93/Functions/extract.table.R#L59 and the scaling logic happens here: https://github.com/TGuillerme/eco_metrics_simulations/blob/c2a8deb9d673e0456f315d9a9f43285aafdfea93/Functions/extract.table.R#L30 (l.30 to 32)).
  So it might be something with the bandwidth? The whole calculations for the TPD metrics happens here (https://github.com/TGuillerme/eco_metrics_simulations/blob/c2a8deb9d673e0456f315d9a9f43285aafdfea93/Functions/simulation.pipeline.R#L262) with no obvious bandwidth part (at least to my eyes). So maybe it's just using the defaults?

-A minor comment regarding plots:
 - [ ] TODO: reorganise plots labels (follow Stefano's pdf)
 - [ ] TODO: reorganise colours by "Richness / Divergence / Regularity"
 - [ ] TODO: change Convex.hull = Richness; Rao/mpd = Divergence

-Thomas, I would be happy to make the PCAs if you could send me a file with the correlation matrices
 
 - [ ] TODO: get the scaled results matrices for PCA





Let's talk about the two points on the expectations and the biological realism!



<!-- Carlos comments:

-Sorry to insist on this, but I really disagree with the expectations table as it is, we really would have to discuss what are our expectations (if any!) for any particular mechanism, and what they are based on. This is a big problem for me, since it can give the idea that some particular method (or even all methods) is somehow flawed because it doesnt meet some expectation.

-Biological realism: I think the best at this point would be to use some agent-based model as Pedro suggested. Sure, things interact, but I think we can make a model simple enough and change some parameters on it, so that we can isolate more or less different things. We recently made one for a paper, and I would be happy to adapt the code and run some simulations (after we decide which things we want to play with): https://onlinelibrary.wiley.com/doi/epdf/10.1111/geb.13203

 -->