---
title: "Results"
author: "Thomas Guillerme"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_width: 12
    fig_height: 6
---

This script is the repeatable script for measuring changes in the different diversity metrics and how they are affected by the different stressors. 
In this section we are going to analyse the results from the `optim.replicates` chain

```{r}
## Loading the results
load("../Data/Processed/sim_equalizing.rda")
equalizing <- bkp
load("../Data/Processed/sim_facilitation.rda")
facilitation <- bkp
load("../Data/Processed/sim_filtering.rda")
filtering <- bkp
load("../Data/Processed/sim_competition.rda")
competition <- bkp
```

```{r}
## Loading the functions
source("../Functions/summarise.results.R")
source("../Functions/plot.metric.results.R")
```

# Results per metrics

Here I'm going to display the results in box plot for each metric in each broad category.
> We can probably think of a better way to display the results in the future.

For each metric displayed here:
 
 * The different colours correspond to the different stressors (equalizing, facilitation, filtering and competition)
 * The different density of each colours correspond to the different levels of each stressor (removing 20, 40, 60 or 80% of the data)
 * All the metrics values are centred on the null (random) stressor for each individual simulation (i.e. 1 dataset is simulated, X% of the data is removed, one time randomly, one time biased - following the stressor; the metric is calculated on both the random and the biased removal and the biased removal is then centred on the random one (`biased metric score - random metric score`) for each simulation).
 * All the centred metric values are then scaled to the absolute maximum change (`centred scores/max(abs(centred scores))`).

```{r}
## Creating the list of results
all_results <- list(equalizing   = equalizing,
                    facilitation = facilitation,
                    filtering    = filtering,
                    competition  = competition)
## The different removal level names
removals <- c("20%", "40%", "60%", "80%")
## The colours
cols <- c("blue", "orange", "red", "green")
```

# TODOs:

  1. reworking the scaling 
    > Carlos: I suspect the standardization you made removes part of the signal, since you are standardizing within each level of each stressor, right? The way I see it, this probably "smoothes out" the differences between levels of the same stressor. I would use the maximum absolute change of a method for a stressor ACROSS ALL LEVELS as the value to standardize. But maybe I am wrong, or maybe I did not understand it correctly
    `TODO: work on scale function`
  2. plotting the metrics per stressors (i.e. all metrics for each stressor).
    `TODO: re-work the plots (15 rows * 4 columns; each row is a metric and contains the lines for the 4 rarefactions; each column is a stressor)`
  3. fitting a linear model per metric per stressor to rank models based on their r2
    > Pedro: Wonder if it makes sense to fit linear (or other) models to each trend (20, 40, 60, 80) and the higher the r2 the higher the clearer or more probable the detection of the trend?
  4. t-test of difference between each random score and the stressor
    `TODO 3+4: results table per metric`

  5. run a psych-style pairwise comparison between each scaled metric
  6. put the results in the context of the expectation table from the paper (Null vs process.docx)




## Tree metrics

### Alpha

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "alpha", level.names = removals, col = cols)
```

### Dispersion

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "dispersion", level.names = removals, col = cols)
```

### Evenness

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "evenness", level.names = removals, col = cols, legend.pos = "bottomright")
```

## Dissimilarity metrics 

### Rao

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "FD_Rao", level.names = removals, col = cols, legend.pos = "bottomright")
```

### Divergence (FD)

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "FD_divergence", level.names = removals, col = cols, legend.pos = "bottomright")
```

### Evenness (FD)

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "FD_evenness", level.names = removals, col = cols, legend.pos = "bottomright")
```

### Melodic Rao

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "melodic_Rao", level.names = removals, col = cols)
```

### Melodic MPD

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "melodic_mpd", level.names = removals, col = cols)
```         

## Convex hull metrics

### Convex hull alpha

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "convex.hull", level.names = removals, col = cols)
```  

## Hypervolume metrics

### Richness

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "hypervolume_richness", level.names = removals, col = cols)
``` 

### Dispersion

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "hypervolume_dispersion", level.names = removals, col = cols)
``` 

### Regularity

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "hypervolume_regularity", level.names = removals, col = cols)
``` 

## Probability density metrics

### Richness

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "TPD_Richness", level.names = removals, col = cols)
``` 

### Evenness

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "TPD_Evenness", level.names = removals, col = cols)
``` 

### Divergence

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "TPD_Divergence", level.names = removals, col = cols)
``` 

# My interpretation

 * The fact that most metrics pick up a negative change is interesting: a random decrease in number of elements often results in a smaller decrease in metric score than a non-random decrease in number of elements. In other words reducing X% of the elements in a biased way often reduces the score of all the metrics compared to a non-biased way. This is promising, if someone uses any metric to measure a decrease in something, it's better that it is due to a non-random cause rather than a random one ;).
 * The fact that different levels of removal (20%, 40%, etc...) results in different distributions of the metrics is also a good check (the level of the removal seems to often impact either the variance or the mean of the metric score - or both).
 * The fact that we can roughly see differences between the response of the metric and the different stressors is also encouraging (e.g. TPD divergence seems to pick up differences in scenarios pretty neatly!).
 * And finally, one that seems to be clearly working is the FD_evenness metric which seems to pick up changes in facilitation pretty neatly (if not too many species are removed).









<!-- # Sanity checking

This is what is happening in one simulation (e.g. for alpha)
 -->

```{r, eval = FALSE, echo = FALSE}
## Get the result table
results_table <- facilitation$results_table

## Get the alpha metric
alpha_results <- results_table[, grep("alpha", colnames(results_table))]

## Get the results of the first simulation
first_sim_alpha <- alpha_results[1,]

## Get the data removal from the first simulation for 40% removal
first_sim <- facilitation$output_save[[1]]
random_sim_40 <- first_sim$reductions[[2]][, "random"]
stress_sim_40 <- first_sim$reductions[[2]][, "stressor"]

par(mfrow = c(1, 2))
plot(first_sim$trait_space, col = "grey", pch = 19, main = "stressor")
points(first_sim$trait_space[stress_sim_40, ], col = "orange", pch = 19)
plot(first_sim$trait_space, col = "grey", pch = 19, main = "random")
points(first_sim$trait_space[random_sim_40, ], col = "blue", pch = 19)

## Recalculate the metrics
tree <- hclust(dist(first_sim$trait_space), method = "average")
BAT::alpha(t(first_sim$reductions[[2]]), tree)
#          Richness
# random   65.29787
# stressor 46.76265
first_sim_alpha[3:4] # OK
```

