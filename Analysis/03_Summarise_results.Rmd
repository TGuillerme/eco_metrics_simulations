---
title: "Results"
author: "Thomas Guillerme"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_width: 12
    fig_height: 6
---

This script is the repeatable script for measuring changes in the different diversity metrics and how they are affected by the different stressors. 
In this section we are going to analyse the results from the `optim.replicates` chain

```{r, echo = FALSE}
library(knitr)
## Loading the results
load("../Data/Processed/sim_equalizing.rda")
equalizing <- bkp
load("../Data/Processed/sim_facilitation.rda")
facilitation <- bkp
load("../Data/Processed/sim_filtering.rda")
filtering <- bkp
load("../Data/Processed/sim_competition.rda")
competition <- bkp

## Loading the functions
source("../Functions/extract.table.R")
source("../Functions/plot.metric.results.R")
source("../Functions/paired.tests.R")
source("../Functions/split.metric.R")
source("../Functions/fit.model.rm.R")
source("../Functions/fancy.table.R")
source("../Functions/fancy.plot.R")
source("../Functions/fancy.names.R")
```

# Results per metrics

Here I'm going to display the results in box plot for each metric in each broad category.
> We can probably think of a better way to display the results in the future.

For each metric displayed here:
 
 * The different colours correspond to the different stressors (equalizing, facilitation, filtering and competition)
 * The different density of each colours correspond to the different levels of each stressor (removing 20, 40, 60 or 80% of the data)
 * All the metrics values are centred on the null (random) stressor for each individual simulation (i.e. 1 dataset is simulated, X% of the data is removed, one time randomly, one time biased - following the stressor; the metric is calculated on both the random and the biased removal and the biased removal is then centred on the random one (`biased metric score - random metric score`) for each simulation).
 * All the centred metric values are then scaled to the absolute maximum change (`centred scores/max(abs(centred scores))`).

```{r, echo = FALSE}
## Creating the list of results
all_results <- list(equalizing   = equalizing,
                    facilitation = facilitation,
                    filtering    = filtering,
                    competition  = competition)
## The different removal level names
removals <- c("20%", "40%", "60%", "80%")
## The colours
cols <- c("blue", "orange", "red", "green")
```

## Scaling and centring the results

We centre the metric based on the null results by substracting the score of the metric from the randomly reduced trait space to the stressed trait space (i.e. $score_{centred} = score_{stressed} - score_{random}$).
We then scale the centred metric by dividing it by it's maximum absolute centred value between all stressors (i.e. $score_{scaled} = score_{centred} / \text{max}(|scores_{centred}|))$).

```{r, echo = FALSE}
## Extracting all the scaled results
scaled_results <- extract.table(all_results, scale.method = "between")
scaled_within <- extract.table(all_results, scale.method = "within")
```

## Measuring the statistics for each metric

To measure the differences between the random and stressed metric scores, we ran paired t-tests between each pairs of unscaled metric score for the random reduction of the trait space and the stressed reduction of the trait space (`t.test(random_scores, stressor_scores, paired = TRUE)`).

To check the trend in the different level of data removals (20, 40, 60 and 80%) using a linear model with the $scores_{scaled}$ as a response to the different removal levels (`lm(scaled_metric ~ removal_level`).

```{r, echo = FALSE}
## Slope and R2 for each model
model_fits <- lapply(scaled_results, fit.model.rm)
## Differences between stressor and random
null_differences <- lapply(all_results, paired.tests)

## Making the fancy tables
model_table <- fancy.table(model_fits, columns = list(c(3,4), 5))
null_table <- fancy.table(null_differences, columns = list(c(1,3), c(4, 6), c(7, 9)))
```

```{r, echo = FALSE}
kable(model_table, caption = "Results of the model scaled metric ~ removal level per stressor", align = "c")
```

```{r, echo = FALSE}
## Even fancier column names
colnames(null_table) <- gsub(".d_rm", ".rm", colnames(null_table))
colnames(null_table) <- gsub("equalizing", "equa", colnames(null_table))
colnames(null_table) <- gsub("facilitation", "faci", colnames(null_table))
colnames(null_table) <- gsub("filtering", "filt", colnames(null_table))
colnames(null_table) <- gsub("competition", "comp", colnames(null_table))
kable(null_table, caption = "Average difference between the null metric and the stressed metric (raw) for each level of removal and each stressor", align = "c")
```

# Plots per stressor

```{r, fig.width = 8, fig.height = 10, echo = FALSE}
fancy.plot(scaled_results, col = c("blue", "orange"))
```

Figure X1: Simulation results: the y axes represent the different metrics tested (sorted by categories). The different columns represent the different stressors. The x-axes represent the metric values centred on the random changes and scaled by the maximum value for each metric between the four stressors. Negative and positive values signify a decrease/increase in the metric score. The dots represent the median metric value, the full line their 50% confidence interval (CI) and the dashed line their 95% CI. The colours are just here to visually separate the metrics rows but the colour gradient within each row corresponds to a removal of respectively 80%, 60%, 40% and 20% of the data (from top to bottom).

```{r, fig.width = 8, fig.height = 10, echo = FALSE}
fancy.plot(scaled_results, col = c("blue", "orange"), lm = model_fits, null = null_differences)
```

Figure X2: See above + Grey lines in the background are a fitted linear model on the scaled metric score function of removal amount and the value displayed is the adjusted R^2 from each of these models. Dashed grey lines represent non-significant models (slope or/and intercept). The grey line plots represent (CI + median) represent distribution of metrics scores not clearly different from the random metric scores (paired t-test p value > 0.05).

> I think that Figure X2 would do a good summary of how the metrics pick up changes. Here's a couple of interpretations of it in the light of my previous work on which metric to choose:
  * Don't use the TPD richness metric, it doesn't do much.
  * On the other hand, hypervolume regularity and richness are two good metrics because they only pick up really specific changes (respectively "equalizing" and "facilitation") and nothing else.


# Results interpretations in the light of our expectation table

These were our expectations:

Mechanism | Richness| Dispersion | Regularity
-------------|-------|-------|-------|
Equalizing   | Lower | Lower | Higher|
Facilitation | Higher| Higher| Higher|
Filtering (exclusion) | Higher| Higher| Higher|
Competition  | Lower | Lower |Nothing|

And these are our results per metric family: 

 * [ok] prediction is correct (lower = decrease; higher = increase)
 * [-ok] prediction is kinda correct (there is a change compared to the null but not in the right direction)
 * [NO] to a wrong prediction: no change when change is expected or change when no change is expected

> NOTE: the difference between [ok] and [-ok] is actually not really meaningful here since these graphs shows differences compared to the null results so if both the null and stressor results increase but the stressor increases slowlier than the stressor, the results in the table will decrease. Although in general, if this is the case, this is a "bad" results since ideally we would want the null to not change at all (i.e. if the metric is just picking up changes in number of elements, this is not really useful).

#### Tree (dissimilarity)

Mechanism    | Richness (alpha)| Dispersion | Regularity
-------------|------------|-------------|-------|
Equalizing   | [ok] Lower | [ok] Lower  | [-ok] Higher|
Facilitation | [NO] Higher| [-ok] Higher| [-ok] Higher|
Exclusion    | [NO] Higher| [-ok] Higher| [-ok] Higher|
Competition  | [ok] Lower | [ok] Lower  | [NO] Nothing|

#### FD

Mechanism | Richness (Rao)| Dispersion | Regularity
-------------|-------|-------|-------|
Equalizing   | [NO] Lower  | [ok] Lower | [-ok] Higher|
Facilitation | [NO] Higher | [-ok] Higher| [NO] Higher|
Exclusion    | [-ok] Higher| [-ok] Higher| [-ok] Higher|
Competition  | [ok] Lower  | [ok] Lower | [NO] Nothing|

#### hypervolume

Mechanism | Richness (Rao)| Dispersion | Regularity
-------------|-------|-------|-------|
Equalizing   | [NO] Lower  | [-ok] Lower | [-ok] Higher|
Facilitation | [-ok] Higher| [NO] Higher| [NO] Higher|
Exclusion    | [NO] Higher | [NO] Higher| [NO] Higher|
Competition  | [ok] Lower  | [NO] Lower | [ok] Nothing|


#### TPD

Mechanism | Richness (Rao)| Dispersion | Regularity
-------------|-------|-------|-------|
Equalizing   | [NO] Lower  | [ok] Lower | [-ok] Higher|
Facilitation | [NO] Higher | [-ok] Higher| [-ok] Higher|
Exclusion    | [NO] Higher | [-ok] Higher| [-ok] Higher|
Competition  | [NO] Lower  | [ok] Lower | [NO] Nothing|

#### Other

I actually don't remember of the top of my head what the convex.hull and melodic rao/mpd are representing.






# Metrics correlations

And finally below are all the correlations per stressor between each metric per removal levels (20, 40, 60, 80%).

> TODO: do the PCA plots as suggested by Carlos to see things clearlier.


```{r, eval = TRUE, echo = FALSE, message = FALSE}
require(psych)

## Split the results per stressor and per metric
## (to extract the different removal levels)
splitted_results <- lapply(scaled_results, split.metric)

## Get the right removal level
get.matrix.level <- function(results, level = 1) {
    do.call(cbind, lapply(results, function(X, level) return(X[, level, drop = FALSE]), level = level))
}
## Wrapper for pairs.panels (with fancy names)
pairs.panels.wrapper <- function(results, ...) {
    ## Make fancy names
    colnames(results) <- fancy.names(colnames(results), sep = ":\n")
    psych::pairs.panels(results, ...)
}

```

## Equalizing

### Metrics correlation for equalizing with 20% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$equalizing, level = 1))
```

### Metrics correlation for equalizing with 40% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$equalizing, level = 2))
```

### Metrics correlation for equalizing with 60% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$equalizing, level = 3))
```

### Metrics correlation for equalizing with 80% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$equalizing, level = 4))
```

## Facilitation

### Metrics correlation for facilitation with 20% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$facilitation, level = 1))
```

### Metrics correlation for facilitation with 40% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$facilitation, level = 2))
```

### Metrics correlation for facilitation with 60% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$facilitation, level = 3))
```

### Metrics correlation for facilitation with 80% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$facilitation, level = 4))
```

## Filtering

### Metrics correlation for filtering with 20% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$filtering, level = 1))
```

### Metrics correlation for filtering with 40% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$filtering, level = 2))
```

### Metrics correlation for filtering with 60% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$filtering, level = 3))
```

### Metrics correlation for filtering with 80% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$filtering, level = 4))
```

## Competition

### Metrics correlation for competition with 20% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$competition, level = 1))
```

### Metrics correlation for competition with 40% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$competition, level = 2))
```

### Metrics correlation for competition with 60% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$competition, level = 3))
```

### Metrics correlation for competition with 80% removal

```{r, fig.height = 18, fig.width = 18, echo = FALSE, eval = TRUE}
## Equalizing level 1
pairs.panels.wrapper(get.matrix.level(splitted_results$competition, level = 4))
```













# Responses

Carlos:
-TPD richness potential error: 
  * I have double checked the scaling and everything seems OK (but I might be mistaken!) The centreing logic happens here: https://github.com/TGuillerme/eco_metrics_simulations/blob/c2a8deb9d673e0456f315d9a9f43285aafdfea93/Functions/extract.table.R#L59 and the scaling logic happens here: https://github.com/TGuillerme/eco_metrics_simulations/blob/c2a8deb9d673e0456f315d9a9f43285aafdfea93/Functions/extract.table.R#L30 (l.30 to 32)).
  So it might be something with the bandwidth? The whole calculations for the TPD metrics happens here (https://github.com/TGuillerme/eco_metrics_simulations/blob/c2a8deb9d673e0456f315d9a9f43285aafdfea93/Functions/simulation.pipeline.R#L262) with no obvious bandwidth part (at least to my eyes). So maybe it's just using the defaults?

-A minor comment regarding plots:
 - [ ] TODO: reorganise plots labels (follow Stefano's pdf)
 - [ ] TODO: reorganise colours by "Richness / Divergence / Regularity"
 - [ ] TODO: change Convex.hull = Richness; Rao/mpd = Divergence

-Thomas, I would be happy to make the PCAs if you could send me a file with the correlation matrices
 
 - [ ] TODO: get the scaled results matrices for PCA





Let's talk about the two points on the expectations and the biological realism!



<!-- Carlos comments:

-Sorry to insist on this, but I really disagree with the expectations table as it is, we really would have to discuss what are our expectations (if any!) for any particular mechanism, and what they are based on. This is a big problem for me, since it can give the idea that some particular method (or even all methods) is somehow flawed because it doesnt meet some expectation.

-Biological realism: I think the best at this point would be to use some agent-based model as Pedro suggested. Sure, things interact, but I think we can make a model simple enough and change some parameters on it, so that we can isolate more or less different things. We recently made one for a paper, and I would be happy to adapt the code and run some simulations (after we decide which things we want to play with): https://onlinelibrary.wiley.com/doi/epdf/10.1111/geb.13203

 -->