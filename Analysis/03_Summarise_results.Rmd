---
title: "Results"
author: "Thomas Guillerme"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_width: 12
    fig_height: 6
---

This script is the repeatable script for measuring changes in the different diversity metrics and how they are affected by the different stressors. 
In this section we are going to analyse the results from the `optim.replicates` chain

```{r}
library(knitr)
## Loading the results
load("../Data/Processed/sim_equalizing.rda")
equalizing <- bkp
load("../Data/Processed/sim_facilitation.rda")
facilitation <- bkp
load("../Data/Processed/sim_filtering.rda")
filtering <- bkp
load("../Data/Processed/sim_competition.rda")
competition <- bkp
```

```{r}
## Loading the functions
source("../Functions/extract.table.R")
source("../Functions/plot.metric.results.R")
source("../Functions/paired.tests.R")
source("../Functions/split.metric.R")
source("../Functions/fit.model.rm.R")
source("../Functions/fancy.table.R")
source("../Functions/fancy.plot.R")
```

# Results per metrics

Here I'm going to display the results in box plot for each metric in each broad category.
> We can probably think of a better way to display the results in the future.

For each metric displayed here:
 
 * The different colours correspond to the different stressors (equalizing, facilitation, filtering and competition)
 * The different density of each colours correspond to the different levels of each stressor (removing 20, 40, 60 or 80% of the data)
 * All the metrics values are centred on the null (random) stressor for each individual simulation (i.e. 1 dataset is simulated, X% of the data is removed, one time randomly, one time biased - following the stressor; the metric is calculated on both the random and the biased removal and the biased removal is then centred on the random one (`biased metric score - random metric score`) for each simulation).
 * All the centred metric values are then scaled to the absolute maximum change (`centred scores/max(abs(centred scores))`).

```{r}
## Creating the list of results
all_results <- list(equalizing   = equalizing,
                    facilitation = facilitation,
                    filtering    = filtering,
                    competition  = competition)
## The different removal level names
removals <- c("20%", "40%", "60%", "80%")
## The colours
cols <- c("blue", "orange", "red", "green")
```

# TODOs:

  * [x]1. reworking the scaling
  * [x]2. plotting the metrics per stressors (i.e. all metrics for each stressor).
    `TODO: re-work the plots (15 rows * 4 columns; each row is a metric and contains the lines for the 4 rarefactions; each column is a stressor)`
  * [x]3. fitting a linear model per metric per stressor to rank models based on their r2
  * [x]4. t-test (paired) of difference between each random score and the stressor
  * [ ]5. run a psych-style pairwise comparison between each scaled metric
  * [ ]6. put the results in the context of the expectation table from the paper (Null vs process.docx)

## Scaling and centring the results

> TODO: we centre like this
> TODO: we scale like this

```{r}
## Extracting all the scaled results
scaled_results <- extract.table(all_results, scale.method = "between")
scaled_within <- extract.table(all_results, scale.method = "within")
```

## Measuring the statistics for each metric

> TODO: we measure paired difference between random and stressor on the raw data
> TODO: we fitted a lm on the scaled data (scaled_metric ~ removal_level)

```{r}
## Slope and R2 for each model
model_fits <- lapply(scaled_results, fit.model.rm)
## Differences between stressor and random
null_differences <- lapply(all_results, paired.tests)

## Making the fancy tables
model_table <- fancy.table(model_fits, columns = list(c(3,4), 5))
null_table <- fancy.table(null_differences, columns = list(c(1,3), c(4, 6), c(7, 9)))
```

```{r, echo = FALSE}
kable(model_table, caption = "Results of the model scaled metric ~ removal level per stressor", align = "c")
```

```{r, echo = FALSE}
## Even fancier column names
colnames(null_table) <- gsub(".d_rm", ".rm", colnames(null_table))
colnames(null_table) <- gsub("equalizing", "equa", colnames(null_table))
colnames(null_table) <- gsub("facilitation", "faci", colnames(null_table))
colnames(null_table) <- gsub("filtering", "filt", colnames(null_table))
colnames(null_table) <- gsub("competition", "comp", colnames(null_table))
kable(null_table, caption = "Average difference between the null metric and the stressed metric (raw) for each level of removal and each stressor", align = "c")
```

# Plots per stressor

```{r, fig.width = 8, fig.height = 12}
fancy.plot(scaled_results, col = c("blue", "orange"))
```

Here is the visualisation of the results per stressor and per metric.




# Plots per metric

## Tree metrics

### Alpha

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "tree_alpha", scale.method = "within", level.names = removals, col = cols)
```

### Dispersion

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "tree_dispersion", scale.method = "within", level.names = removals, col = cols)
```

### Evenness

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "tree_evenness", scale.method = "within", level.names = removals, col = cols, legend.pos = "bottomright")
```

## Dissimilarity metrics 

### Rao

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "FD_Rao", scale.method = "within", level.names = removals, col = cols, legend.pos = "bottomright")
```

### Divergence (FD)

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "FD_divergence", scale.method = "within", level.names = removals, col = cols, legend.pos = "bottomright")
```

### Evenness (FD)

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "FD_evenness", scale.method = "within", level.names = removals, col = cols, legend.pos = "bottomright")
```

### Melodic Rao

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "melodic_Rao", scale.method = "within", level.names = removals, col = cols)
```

### Melodic MPD

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "melodic_mpd", scale.method = "within", level.names = removals, col = cols)
```         

## Convex hull metrics

### Convex hull alpha

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "convex.hull", scale.method = "within", level.names = removals, col = cols)
```  

## Hypervolume metrics

### Richness

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "hypervolume_richness", scale.method = "within", level.names = removals, col = cols)
``` 

### Dispersion

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "hypervolume_dispersion", scale.method = "within", level.names = removals, col = cols)
``` 

### Regularity

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "hypervolume_regularity", scale.method = "within", level.names = removals, col = cols)
``` 

## Probability density metrics

### Richness

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "TPD_Richness", scale.method = "within", level.names = removals, col = cols)
``` 

### Evenness

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "TPD_Evenness", scale.method = "within", level.names = removals, col = cols)
``` 

### Divergence

```{r, fig.height = 4, fig.width = 12}
plot.metric.results(all_results, metric = "TPD_Divergence", scale.method = "within", level.names = removals, col = cols)
``` 

# My interpretation

 * The fact that most metrics pick up a negative change is interesting: a random decrease in number of elements often results in a smaller decrease in metric score than a non-random decrease in number of elements. In other words reducing X% of the elements in a biased way often reduces the score of all the metrics compared to a non-biased way. This is promising, if someone uses any metric to measure a decrease in something, it's better that it is due to a non-random cause rather than a random one ;).
 * The fact that different levels of removal (20%, 40%, etc...) results in different distributions of the metrics is also a good check (the level of the removal seems to often impact either the variance or the mean of the metric score - or both).
 * The fact that we can roughly see differences between the response of the metric and the different stressors is also encouraging (e.g. TPD divergence seems to pick up differences in scenarios pretty neatly!).
 * And finally, one that seems to be clearly working is the FD_evenness metric which seems to pick up changes in facilitation pretty neatly (if not too many species are removed).









<!-- # Sanity checking

This is what is happening in one simulation (e.g. for alpha)
 -->

```{r, eval = FALSE, echo = FALSE}
## Get the result table
results_table <- facilitation$results_table

## Get the alpha metric
alpha_results <- results_table[, grep("alpha", colnames(results_table))]

## Get the results of the first simulation
first_sim_alpha <- alpha_results[1,]

## Get the data removal from the first simulation for 40% removal
first_sim <- facilitation$output_save[[1]]
random_sim_40 <- first_sim$reductions[[2]][, "random"]
stress_sim_40 <- first_sim$reductions[[2]][, "stressor"]

par(mfrow = c(1, 2))
plot(first_sim$trait_space, col = "grey", pch = 19, main = "stressor")
points(first_sim$trait_space[stress_sim_40, ], col = "orange", pch = 19)
plot(first_sim$trait_space, col = "grey", pch = 19, main = "random")
points(first_sim$trait_space[random_sim_40, ], col = "blue", pch = 19)

## Recalculate the metrics
tree <- hclust(dist(first_sim$trait_space), method = "average")
BAT::alpha(t(first_sim$reductions[[2]]), tree)
#          Richness
# random   65.29787
# stressor 46.76265
first_sim_alpha[3:4] # OK
```

