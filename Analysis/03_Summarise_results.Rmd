---
title: "Results"
author: "Thomas Guillerme"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_width: 12
    fig_height: 6
---

This script is the repeatable script for measuring changes in the different diversity metrics and how they are affected by the different stressors. 
In this section we are going to analyse the results from the `optim.replicates` chain

```{r}
library(knitr)
## Loading the results
load("../Data/Processed/sim_equalizing.rda")
equalizing <- bkp
load("../Data/Processed/sim_facilitation.rda")
facilitation <- bkp
load("../Data/Processed/sim_filtering.rda")
filtering <- bkp
load("../Data/Processed/sim_competition.rda")
competition <- bkp
```

```{r}
## Loading the functions
source("../Functions/extract.table.R")
source("../Functions/plot.metric.results.R")
source("../Functions/paired.tests.R")
source("../Functions/split.metric.R")
source("../Functions/fit.model.rm.R")
source("../Functions/fancy.table.R")
source("../Functions/fancy.plot.R")
```

# Results per metrics

Here I'm going to display the results in box plot for each metric in each broad category.
> We can probably think of a better way to display the results in the future.

For each metric displayed here:
 
 * The different colours correspond to the different stressors (equalizing, facilitation, filtering and competition)
 * The different density of each colours correspond to the different levels of each stressor (removing 20, 40, 60 or 80% of the data)
 * All the metrics values are centred on the null (random) stressor for each individual simulation (i.e. 1 dataset is simulated, X% of the data is removed, one time randomly, one time biased - following the stressor; the metric is calculated on both the random and the biased removal and the biased removal is then centred on the random one (`biased metric score - random metric score`) for each simulation).
 * All the centred metric values are then scaled to the absolute maximum change (`centred scores/max(abs(centred scores))`).

```{r}
## Creating the list of results
all_results <- list(equalizing   = equalizing,
                    facilitation = facilitation,
                    filtering    = filtering,
                    competition  = competition)
## The different removal level names
removals <- c("20%", "40%", "60%", "80%")
## The colours
cols <- c("blue", "orange", "red", "green")
```

# TODOs:

  * [x]1. reworking the scaling
  * [x]2. plotting the metrics per stressors (i.e. all metrics for each stressor).
    `TODO: re-work the plots (15 rows * 4 columns; each row is a metric and contains the lines for the 4 rarefactions; each column is a stressor)`
  * [x]3. fitting a linear model per metric per stressor to rank models based on their r2
  * [x]4. t-test (paired) of difference between each random score and the stressor
  * [ ]5. run a psych-style pairwise comparison between each scaled metric
  * [ ]6. put the results in the context of the expectation table from the paper (Null vs process.docx)

## Scaling and centring the results

> TODO: we centre like this
> TODO: we scale like this

```{r}
## Extracting all the scaled results
scaled_results <- extract.table(all_results, scale.method = "between")
scaled_within <- extract.table(all_results, scale.method = "within")
```

## Measuring the statistics for each metric

> TODO: we measure paired difference between random and stressor on the raw data
> TODO: we fitted a lm on the scaled data (scaled_metric ~ removal_level)

```{r}
## Slope and R2 for each model
model_fits <- lapply(scaled_results, fit.model.rm)
## Differences between stressor and random
null_differences <- lapply(all_results, paired.tests)

## Making the fancy tables
model_table <- fancy.table(model_fits, columns = list(c(3,4), 5))
null_table <- fancy.table(null_differences, columns = list(c(1,3), c(4, 6), c(7, 9)))
```

```{r, echo = FALSE}
kable(model_table, caption = "Results of the model scaled metric ~ removal level per stressor", align = "c")
```

```{r, echo = FALSE}
## Even fancier column names
colnames(null_table) <- gsub(".d_rm", ".rm", colnames(null_table))
colnames(null_table) <- gsub("equalizing", "equa", colnames(null_table))
colnames(null_table) <- gsub("facilitation", "faci", colnames(null_table))
colnames(null_table) <- gsub("filtering", "filt", colnames(null_table))
colnames(null_table) <- gsub("competition", "comp", colnames(null_table))
kable(null_table, caption = "Average difference between the null metric and the stressed metric (raw) for each level of removal and each stressor", align = "c")
```

# Plots per stressor

```{r, fig.width = 8, fig.height = 10, echo = FALSE}
fancy.plot(scaled_results, col = c("blue", "orange"))
```

Figure X1: Simulation results: the y axes represent the different metrics tested (sorted by categories). The different columns represent the different stressors. The x-axes represent the metric values centred on the random changes and scaled by the maximum value for each metric between the four stressors. Negative and positive values signify a decrease/increase in the metric score. The dots represent the median metric value, the full line their 50% confidence interval (CI) and the dashed line their 95% CI. The colours are just here to visually separate the metrics rows but the colour gradient within each row corresponds to a removal of respectively 80%, 60%, 40% and 20% of the data (from top to bottom).

>TG: Here is the visualisation of the results per stressor and per metric.

```{r, fig.width = 8, fig.height = 10, echo = FALSE}
fancy.plot(scaled_results, col = c("blue", "orange"), lm = model_fits, null = null_differences)
```

Figure X2: See above + Grey lines in the background are a fitted linear model on the scaled metric score function of removal amount and the value displayed is the adjusted R^2 from each of these models. Dashed grey lines represent non-significant models (slope or/and intercept). The grey line plots represent (CI + median) represent distribution of metrics scores not clearly different from the random metric scores (paired t-test p value > 0.05).

>TG: Here is the visualisation of the results per stressor and per metric.









# My interpretation

 * The fact that most metrics pick up a negative change is interesting: a random decrease in number of elements often results in a smaller decrease in metric score than a non-random decrease in number of elements. In other words reducing X% of the elements in a biased way often reduces the score of all the metrics compared to a non-biased way. This is promising, if someone uses any metric to measure a decrease in something, it's better that it is due to a non-random cause rather than a random one ;).
 * The fact that different levels of removal (20%, 40%, etc...) results in different distributions of the metrics is also a good check (the level of the removal seems to often impact either the variance or the mean of the metric score - or both).
 * The fact that we can roughly see differences between the response of the metric and the different stressors is also encouraging (e.g. TPD divergence seems to pick up differences in scenarios pretty neatly!).
 * And finally, one that seems to be clearly working is the FD_evenness metric which seems to pick up changes in facilitation pretty neatly (if not too many species are removed).


